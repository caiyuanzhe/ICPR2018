\section{Co-channel Speech SRE Framework}
\label{sec:algorithm}

\subsection{Motivation}
  
As we know, the quality of the user's audio is very important for the speaker verification.  Hence, we need to design a good method to extract the customer's voice from the mix audio. The current Ping An TCS has the following characteristics. First, as the customer service, only two speakers (customer/service staff) are in the system. The binary classification approaches can be applied to identify the customer's audio. Second, the contents of speakers are much different between customer and service. Since our audio come from Ping An Bank (\url{http://bank.Pingan.com/}) customer service, the similar dialogues, such as ``apply the deposit", ``apply the credit" and etc, always appear in the audio set. The similar service¡¯s questions, such as ``What is your surname?", ``May I help you?", etc and the similar customer¡¯s answers, such as ``My surname is ....", ``apply the deposit" repeat for multiple times. Therefore, it is possible to use the ``keyword matching" (e.g., ``My" for customer, ``help" for service, etc) to assign the ``customer" and ``service" label to these audio. Thus, our framework extracts the text from the audio by ASR model and use the text classification approach to identify the customer's content and then splices all the customer's audio together.

\begin{figure}
\centering
\includegraphics[width=3.4in]{figures/PinganSampleTCS.eps}
\caption{a Sample for Ping¡¯an Telephone Customer Service (The customer information has already been removed.) }
\label{fig:PinganSampleTCS}
\end{figure}

\subsection{Overview}
The proposed system is an ensemble of three modules (See in Figure \ref{fig:PinganSRE},) that is: (i) an ASR module for generating recognized text from input mixed audio. (ii) a Customer/Service identification module for classifying speakers and identify the customer's audio files, so that we can get the cleaned one-person single track audio file as input to the recognition module. (iii) a speaker recognition module, specifically a text-independent speaker verification, with the input of split audio of customer, verifying the identity of the speaker. The detail information is shown in the following sections.

\begin{figure*}[htbp]
\centering
\includegraphics[width=4.5in]{figures/PinganSRE.eps}
\caption{SV Framework for designed Co-Channel TCS}
\label{fig:PinganSRE}
\end{figure*}

\subsection{ASR Module}
\label{sec:asrmodule}

The ASR model helps to transcribe co-channel speech to conversational text. The co-channel audio data is often received as a non-overlapped mono audio file although there are two speakers within the audio file. And the audio can be segmented into audio data attributed to separate speakers.

As we wish to send short text to the Customer/Service Identification module module, we first apply Sequential Gaussian Mixture Model based Voice Activity Detection (SGMM-VAD) algorithm, described in \cite{Ying2011Noise}, to split the mixed audio into audio segments. The SGMM-VAD comprises two Gaussian components which respectively describe the speech and non-speech log-power distributions, helps to detect audio parts from noisy speech with high signal noise ratio.

The ASR model is then able to transcribe each audio segments into short text. Specifically, we use Latency-controlled Bidirectional Highway LSTM RNN model (LC-BHLSTM) \cite{Zhang2016Highway} as the acoustic model of our ASR module, to establish the relationship between the input audio signal and the phonemes or other linguistic units that make up speech. Figure \ref{fig:PinganSRE} shows an sample of ASR result. 

For ASR model setup, the front-end feature consists of 80-dimension log Mel Filterbank (FBANK) with a frame length of 25ms. 3-dimension pitch feature (including probability of Voicing (POV) features) is appended to create 83-dimension feature vector. The LC-BHLSTM has 5 layers, with 1024 memory cells together with a 512-node projection layer at each layer¡¯s output. The output targets are 10k dimensional context-dependent tied triphone states (a.k.a. senones,) of which were determined by the HMM-GMM training stages.


\subsection{Customer/Service Identification}

The customer/service identification module uses the supervised learning classification approach, to identify the customer and service label from the transcribed text. In the training stage, we manually marked the ``customer" and ``service" label to build the training set. After training, we can use this text classification model to automatically assign the ``customer" and ``service" label to the given text segments.

Different from the traditional text/documents, the texts extracted from telephone customer service has the following characteristics. First, these texts are very short (the average text length is only about 8.3.) Figure \ref{fig:PinganSampleTCS} shows the sample of Ping An Telephone Customer Service. Each line is the segment of text \footnote{ASR client identifies each segment by person¡¯s talking pause. If a person pause for more than 1s, ASR server will generate a segment.}, e.g., ``Hello, May I help you?" etc. Therefore, to assign the customer/service label to such short text is a real challenge work. Second, the response time of SV should be very short, since Ping¡¯an Telephone Customer Service need to identify the speaker during a phone call (the average length of phone call is about 3.45 minutes.) Therefore, the running time of this classification approach should also be an important factor.

The motivation of our designed method comes from the search engine process, where nowadays if users want to get an idea of something, they input some queries (key words) into these search engine (e.g., google.com, bing.com) to obtain the related information. Like this procedure, the training short-text has been imported into a search engine; the rest short-texts, as the query, are used to search the relevant texts. The label for the text can be easily voted by the searched texts which has the manually marked label.
Figure \ref{fig:ClassificationTCS} shows the framework of customer/service classification. First, system builds the search engine \cite{brin_page_1998} by these training text (extract Chinese word segmentation, the build the reserved index on these text, etc.) Second, the new short text, as the query of the search engine, retrieves the top K related texts from these training texts. Third, system use the k-nearest-neighbor (KNN) \cite{Bishop:2006:PRM:1162264} to vote the label to the input text based on the retrieved results. The short-text's query word can be set the different various weight \cite{Salton1988Term} (e.g., Boolean, TF, TF.IDF.) In experiment section, the accuracy of TF-KNN achieves the better accuracy then the other approach. Leveraging the well studied IR technique, the proposed approach is extremely scale for the large data and easily add the new training text by updating the index.

\begin{figure}
\centering
\includegraphics[width=3.4in]{figures/ClassificationTCS.eps}
\caption{Customer/Server Classification for Telephone Customer Service System}
\label{fig:ClassificationTCS}
\end{figure}

\subsection{SV Module}

The framework of many voice recognition applications in use is a GMM-based i-vector system (See Figure \ref{fig:IVectorSV}(a)). It can be decomposed into four sequential stages: (i) the feature extraction, the collection of sufficient statistics, the extraction of i-vectors and a scoring criterion backend. The feature extraction is a process converts the speech waveform into characteristic parameter, which retains useful speaker information from given speech signal and filters unwanted information such as noise. Parameters like Mel-Frequency Cepstrum Coefficient (MFCC,) Linear Predictive Cepstral Coefficient (LPCC) and perceptual linear prediction (PLP) are often used in feature extraction stage, follow by voice activity detection (VAD.) (ii) The collection of sufficient statistics is a process to calculate the zero-order, first-order, second-order Baum-Welch statistics from a sequence of feature vector from the first stage. These statistics are highly dimensional, which is generated from a large GMM, called Universal Background Model (UBM.) (iii) The extraction of i-vectors in the third stage is a technique that converts the high dimensional statistics into a single low dimension feature vector, which carries only the discriminative characteristic information apart from the other speakers. (iv) Once i-vectors are extracted, a scoring criterion backend is used to make decision that whether accept or reject the person as the request identity. There are three commonly used scoring criterions for voice recognition, cosine distance similarity, linear discriminant analysis (LDA) and probabilistic linear discriminant analysis (PLDA.)

\begin{figure}
\begin{minipage}{1.0\linewidth}
  \centerline{\includegraphics[width=9.0cm]{figures/GMM-UBM.eps}}
  \centerline{(a) GMM-UBM}
\end{minipage}
\vfill
\begin{minipage}{1.0\linewidth}
  \centerline{\includegraphics[width=9.0cm]{figures/DNN-UBM.eps}}
  \centerline{(b) DNN-UBM}
\end{minipage}
\caption{I-Vector based Speaker Verification Framework}
\label{fig:IVectorSV}
\end{figure}

In the DNN-UBM i-vector framework (See Figure \ref{fig:IVectorSV}(b)), the feature extraction stage stays the same as the GMM-UBM i-vector framework. The different part is that the UBM now utilizes senone states to evaluate the acoustic feature space. In the experiment section, we have tested various kinds of deep neural network to find the best prototype model in our DNN-UBM framework. We show that the Highway LSTM based Universal Background Model (HLSTM-UBM) outperforms the other type of neural network based UBM with comprehensive comparison in accuracy and response time.
