\section{Related Work}
\label{sec:relatedwork}

We categorize existing work related to our study into two main categories: speaker verification, speaker diarization.

\noindent \textbf{Speaker Diarization:} Speaker diarization has emerged as an increasingly important and dedicated domain of speech research. This technique has been used in broadcast news (BN) and conference meetings (CM) to identify the speaker according to their biometric features. The current state-of-the-art speaker diarization system fits into two categories: the bottom-up and the top-down methods. The bottom-up approaches \cite{Anguera2006Robust, Nguyen2009The, Nwe2012Speaker}, using hierarchical clustering (HC), training a number of clusters which aims at successively merging and reducing the number of clusters until only one remains for each speaker. In contrast, the top-down approaches \cite{Fredouille2006Technical, Bozonnet2010The, Fredouille2009The} first cluster the audio stream with a single speaker model and successively add new cluster to it until all the speakers are accounted for. Top-down approaches have performed well against the broader filed of other bottom-up entries. Our system also needs the speaker diarization technique to identify the customer's audio, but telephone customer service is different from BN and CM. First, the above-mentioned clustering approaches cannot be applied in our TCS framework. We assume that the two clusters has been identified from the audio, but we still do not know which cluster is customer's audio. Therefore, the binary classification approach has been applied in TCS. Second, Second, the state-of-the-art speaker diarization system mainly uses the speaker's biometric features, such as audio waves, voice pitch, speaking style, etc, but our framework mainly use the speaker's content information, which is totally different from the former work. By the way, the biometric features cannot be used, since it is very difficult to identify some rules or models for these biometric features since these audio from various customers and services. In the other words, we cannot use the audio waves, voice pitch and speaking style to distinguish the customer and service. However, the speaker's content can be used because of the difference speaking characters between customer and service.

\noindent \textbf{Speaker Verification:} So far, the GMM-UBM i-vector based model \cite{dehak2011front} is still the dominant model used in modern speaker verification systems. With the development of deep neural network models in speech recognition, people start to explore using DNN in speaker verification framework. There are two dominant approaches that are the most widely adopted in research for combining deep neural network into speaker recognition. One is to extract deep neural network bottleneck feature from SV feature (i.e. MFCC). As reported in \cite{yamada2013improvement}, one important factor that leads to superior performance improvement in distant-talking recognition is utilizing bottleneck feature to restrain channel-mismatch reverberant conditions. The other is to replace GMM posterior with DNN posterior during feature modeling, so called DNN-UBM \cite{Snyder2016Time, mclaren2015advances, saleem2016discriminative}. This method takes the advantage of deep neural network model ability to directly model phonetic content, rather than an arbitrary acoustic space, achieves a 50\% relative improvement compared to the traditional GMM-UBM method which can well perform than the first approach even more. Therefore, our SV framework uses the DNN-UBM approach, test with the various kinds of DNN models to optimize the verification system performance.
